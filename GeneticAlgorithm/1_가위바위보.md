# 가위바위보
제일 처음 만들었던 유전 알고리즘 모델이다. 가위 또는 바위가 랜덤하게 배치된 환경에 가위,바위,보를 랜덤하게 내는 개체들을 만들고 진화시킨다.  
개체들은 서로 동일한 환경에서 승리한 횟수로 평가받고, 높은 평가를 받은 개체들을 위주로 다음 세대를 구성하는 것이 기본 원리다.  
```python
#상위 30% crossover model

import random
from matplotlib import pyplot as plt

def RSP(env,life):
    if (env=='R' and life=='P') or (env=='S' and life=='R') or (env=='P' and life=='S'):
        return 'win'
    elif (env=='R' and life=='S') or (env=='S' and life=='P') or (env=='P' and life=='R'):
        return 'lose'
    else:
        return 'draw'
    
def main():
    #parameter 설정
    p=eval(input("바위의 확률:"))
    l=eval(input("data set 길이:"))
    num=eval(input("life data set 개수:"))
    gen=eval(input("진행할 세대 수:"))
    mut=eval(input("mutation rate:"))
    
    #확률 적용
    #R=바위, S=가위, P=보
    prob=[]
    for i in range(100):
        if i<p:
            prob.append('R')
        else:
            prob.append('S')
            
    normal = ['R','S','P']
    
    #최초 life data set 구성
    life=[]
    for i in range(num):
        temp=[]
        for j in range(l):
            temp.append(random.choice(normal))
        life.append(temp)

    avg=[]
    top_avg=[]
    process=0
    for i in range(gen):
        env=[]
        scr={}
        for j in range(l):
            env.append(random.choice(prob))     #해당 세대의 env 형성
        temp=0
        for j in life:                          #scoring
            score=0
            for k in range(l):
                result=RSP(env[k],j[k])
                if result=='win':
                    score+=1
                elif result=='lose':
                    score-=1
            j.append(score/l)
            temp+=score/l
            if j[-1] in scr:
                scr[j[-1]].append(j)
            else:
                scr[j[-1]]=[j]
        avg.append(temp/num)                    #average of score
        top_num=0
        temp=0
        top=[]
        scr_list=[]
        for j in scr:
            scr_list.append(j)
        scr_list.sort(reverse=True)
        for j in scr_list:
            n=len(scr[j])
            top_num+=n
            temp+=n*j
            top+=scr[j]
            if top_num>=num/30:
                break
        top_avg.append(temp/top_num)            #average of score of top 30%

        locus=[]                                #gene pool for crossover
        for j in range(int(l/10)+int(not l%10==0)):
            locus.append([])
        for j in top:
            k=0
            while k*10<l:
                locus[k].append(j[10*k:10*(k+1)])
                k+=1
        next_life=[]                            #making next generation by crossover
        for j in range(num):
            temp=[]
            for k in locus:
                temp+=random.choice(k)
            next_life.append(temp)
        
        life=next_life

        mut_num=int(l*mut)                      #give mutation to next generation by mutation rate
        for j in life:
            mut_site=random.sample(range(l),mut_num)
            for k in mut_site:
                j[k]=random.choice(normal)
        
        if i/gen*100>=process:                  #show the percentage of processing
            print(str(process)+'%')
            process+=10
        
    plt.plot(avg)
    plt.xlabel('Generation')
    plt.ylabel('Fitness')
    plt.title('Average')
    plt.ylim(0.0,1.0)
    plt.xlim(0,gen)
    plt.show()
    plt.plot(top_avg)
    plt.xlabel('Generation')
    plt.ylabel('Fitness')
    plt.ylim(0.0,1.0)
    plt.xlim(0,gen)
    plt.title('Average of top 30%')
    plt.show()

main()
```
프로그램을 실행하면 우선 parameter 설정을 입력받는다. 각각에 대한 설명은 코드 흐름과 함께 서술한다.  
parameter인 p 값에 따라 일정 비율의 가위(S)와 바위(R)로 구성된 prob 리스트를 만든다. 나중에 환경 값을 만들 때 이 리스트를 기반으로 한다. 이와 함께 R,S,P가 1:1:1로 존재하는 normal 리스트도 만든다. 이는 랜덤한 개체들을 만들 때 사용된다.  
최초 life data set, 즉 최초 개체군을 만든다. 개체군은 길이 l인 리스트 num개로 구성된다. 각 리스트는 normal에서 추출한 랜덤 값으로 구성된다.  

이제 gen 세대만큼 유전을 반복한다. 마지막에 결과 그래프를 만들기 위해, 매 세대마다 개체들의 평가 수치인 fitness의 평균과 fitness가 상위 30%인 개체들의 평균을 각각 avg와 top_avg에 기록한다. process는 프로그램 진행도를 콘솔에 나타내주기 위한 변수다.  
세대별 작업의 첫 단계는 환경 설정이다. 앞서 만든 prob 리스트를 이용해 개체 리스트의 길이 l과 같은 길이의 환경을 만들어준다. 그리고 개체들을 이 환경과 비교해 점수를 매긴다.  
점수는 개체와 환경을 같은 index의 원소끼리 비교해 개체가 이기면 +1, 비기면 +0, 지면 -1씩을 부여한다. 개체들의 점수 평균을 avg에 기록하고, 점수가 상위 30%인 개체들을 추려 이들의 평균 점수를 top_avg에 기록한다. 상위 30%에 해당하는 개체들을 기반으로 다음 세대를 구성하는데, 이 부분이 적자생존의 원리에 해당한다.  

상위 30% 개체들을 각각 10개의 파편으로 쪼개고, locus[i]에 i번째 조각을 저장한다. 그 후 locus의 원소들에서 파편을 하나씩 랜덤 추출해 이어붙인 다음 세대의 개체를 num개 만든다. 이 부분은 유전자 교차 과정에 해당한다.  

다음 세대의 각 개체들마다 길이 l의 mut 비율만큼 index를 랜덤 추출하여 유전자 값인 R,S,P를 랜덤하게 다시 설정해준다. 이전 세대에게 물려받은 유전 정보에 변형을 가하는 과정으로, 유전자 돌연변이에 해당한다.  

위의 유전 과정을 gen 세대만큼 반복한 후, 기록된 avg와 top_avg를 그래프로 나타냄으로써 프로그램이 종료된다.  

바위의 확률을 0, 개체를 평가할 환경인 data set의 길이를 100, 한 세대를 구성하는 개체의 수를 100, 학습을 진행할 세대 수를 100, 각 세대에서 mutation이 일어날 확률을 0.01로 설정했을 때, 아래 그림과 같은 결과 그래프가 나왔다.  
<p align = center color='black'>
    <img src="https://user-images.githubusercontent.com/62535139/212324854-d734a373-a63b-4d93-af47-594e4002137a.png" width = "49%" align = center>
    <img src="https://user-images.githubusercontent.com/62535139/212325646-078cf617-17f9-457a-a125-fc12d64b9ae7.png" width = "49%" align = center>
</p>      
대략 30세대 정도만에 환경에 적응을 마쳤다. 바위의 확률이 0이기에 환경이 가위로만 구성된 단순한 조건이라 적응이 빠르다.   
위 조건에서 바위의 확률을 50으로 조정하면 결과 양상이 많이 달라진다.  
<p align = center>
    <img src="https://user-images.githubusercontent.com/62535139/212549923-17f38715-b44b-4243-81df-69ed89955b13.png" width = "49%" align = center>
    <img src="https://user-images.githubusercontent.com/62535139/212549925-2b008d21-4d0d-41ee-8d24-194b4cc358e3.png" width = "49%" align = center>
</p>
Fitness가 증가하긴 하지만 50% 부근을 넘어서지 못한다. 환경이 일정하지 않고 50%의 확률로 가위 또는 바위이기 때문에 이전 세대를 통해 아무리 학습해도 50%를 크게 넘어서기 힘든 것이다.  
비슷한 이유로 바위 확률을 30 또는 70으로 주면 fitness가 70 부근에서 진동하게 된다. 모델 구조상 학습 결과가 바위 또는 보를 한 종류만 계속 내는 것으로 수렴하기 때문이다.  

Parameter의 다른 값들을 바꿔주고 결과 변화를 보는 것도 흥미롭다. 예를 들어 data set 길이를 늘려주면, 즉 환경과 개체가 마주치는 횟수를 늘려주면 같은 fitness에 도달하는 데에 더 많은 세대가 걸린다. 이는 각 개체가 가질 수 있는 다양성의 폭이 더 넓어지기 때문에 랜덤 생성과 이들의 조합으로 optima에 도달하기 더 어려워지기 때문일 것이다.  
반대로, life data set 개수를 늘려주면 더 빨리 optima에 도달할 수 있다. 이 역시 한 세대에 다양성이 늘어나기 때문에 매 세대마다 더 넓은 범위를 탐색해서 optima에 빨리 가까워지기 때문으로 보인다.  
결과에 가장 큰 영향을 미치는 parameter는 mutation rate, 돌연변이 확률이다. 이 값이 0이어도, 지나치게 높아도 fitness가 매우 낮은 값을 벗어나지 못하게 된다.  
mutation rate의 목적은 local optima에 빠지지 않게 하는 것이다. 이 값이 0이면 최초에 생성된 랜덤한 개체들의 조합들 밖에 탐색할 수 없어 fitness의 기대값이 낮다. 
하지만 어느 정도 낮은 값으로라도 존재하면 제한된 영역을 벗어나 외부를 탐색함으로써 global optima에 다가갈 기회를 준다.  
그러나 이 값이 너무 높으면 기껏 찾은 optima에서 계속 벗어나기 때문에 역시나 최적화를 방해하는 요소가 된다.  

만들 때는 알고리즘 구현만 생각하느라 신경을 못 썼던 거 같은데, parameter를 입력받을 때 바위의 확률은 % 단위이면서 mutation rate은 0에서 1 사이의 실수인 점이나 input을 받을 때 나타나는 가이드 문구가 직관적이지 않은 점 등 거슬리는 부분들이 많다. 어차피 나만 재미삼아 돌려볼 코드라 괜찮겠지하고 넘어갔었는데 이제 와서 리뷰를 하려니 알아보기 불편했다. 그나마 주석이라도 달아둬서 다행이다.  
그래도 이런 단순한 구조의 데이터 입력과 모델로도 parameter의 영향을 직접 확인할 수 있다는 게 재밌었다. 처음 이 코드를 만들었을 당시에는 결과가 나오는 걸 보고 너무 뿌듯했다. 그래서 본격적으로 입력 데이터에 가중치를 부여하고 결과를 예측하는 학습 모델에도 도전했고, 그 결과물이 이어질 2,3번 글이다.
