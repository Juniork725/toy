# 타이타닉
앞서 만들었던 가위바위보가 각 개체들이 환경에서 무엇을 낼지 유전 알고리즘으로 결정하게 했다면, 이번 모델은 주어진 입력 데이터에 가중치를 어떻게 부여해야 결과를 잘 내는지 결정한다.   
학습할 데이터는 타이타닉 호에 승선했던 사람들의 정보와 생존 여부이다. 성별, 나이, 승선권 가격 등의 데이터에 가중치를 부여해 생존 여부를 예측한다.   
이때 처음에는 랜덤한 가중치를 갖는 모델을 충분히 많이 만들고, 이들의 예측 정확도를 비교해 정확도가 높은 모델들을 위주로 다음 세대를 구성하는 걸 반복한다.   
```python
import random
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

df = pd.read_excel('Titanic.xlsx')

def Normalize(df,label):
    for i in df:
        if not i == label:
            temp = list(df[i])
            max_value = 0
            for j, value in enumerate(temp):
                if not (type(value) == int or type(value) == float):
                    temp[j] = None
                else:
                    if value > max_value:
                        max_value = value
            for j, value in enumerate(temp):
                if not value == None:
                    temp[j] = value/max_value
            df[i] = temp

Normalize(df,df.columns.tolist()[-1])
```
우선 입력 데이터를 읽고 가공한다. Titanic.xlsx의 구조는 아래와 같다.   
|Passenger Class|Sex|Age|No of Siblings|No of parents or children on board|Passenger Fare|Survived|
|---|---|---|---|---|---|---|
|Int|Int(F=0, M=1)|Float|Int|Int|Float|Str(Yes/No)|

순서대로 승객의 등급, 성별, 나이, 형제의 수, 자식 또는 부모의 수, 승선권의 가격, 생존 여부를 나타낸다. 각 column마다 값들의 scale이 다르기에 컬럼별 평균으로 나눠준다. 표준편차로 나눠주면 더 좋지 않았을까 싶다.   
데이터에 일부 값이 누락되어 ?로 표기된 경우가 있다. 이 경우에는 값에 None을 넣어준다.  
```python
I = []
Translator = []
Answer = []
Target = random.sample(range(len(df)), int(len(df)*0.3))  #Target for learning (not full data)

for i in Target:
    data = list(df.iloc[i,:])
    I.append(data[:-1])
    Annotation = data[-1]
    if not Annotation in Translator:
        Translator.append(Annotation)
    Answer.append(Translator.index(Annotation))
Rnum = len(Translator)
```
가공된 입력 데이터의 일부를 학습용으로 사용한다. 일부를 학습해 모델을 만들고 마지막에 전체 데이터에 적용해 성능을 평가하기 위함이다. 여기서는 전체의 30%를 학습했다.   
I는 선택된 30%의 데이터로 구성된 matrix이고, Translator는 str인 label과 모델의 결과값을 연결해주는 역할을 한다. Answer[i]는 I[i]의 정답에 해당하는 label 값을 가진다. Rnum은 정답 label의 가짓수와 같다.  
각 모델은 m\*n 크기의 I에 n\*Rnum 크기의 가중치 matrix를 곱해 m\*Rnum 크기의 결과를 반환한다. 결과의 각 행은 Rnum개의 값 중 가장 큰 값을 예측 결과로 제시하고, 같은 행 번호의 Answer 값과 비교해 정확도를 평가한다.  

```python
def ML(Input, Weight, Rnum):    #From Input, by crossing Weight, give Result length of Rnum.
    Result = []
    for i in range(Rnum):
        r = 0
        for j, value in enumerate(Input):
            r += value*Weight[i][j]
        Result.append(r)
    return Result    #Result of Weighting about Input.

def Weighting(ILength, Rnum):   #Make weighting by random.
    temp = [None]*ILength
    W = []
    for i in range(Rnum):
        W.append(temp[:])
    for i, value in enumerate(W):
        for j in range(len(value)):
            W[i][j] = random.uniform(-1,1)  #Range of each weighting
    return W

def Assess(Result, Answer): #By contrasting Result and Answer, give correct rate.
    count = 0
    for i, value in enumerate(Result):
        if Answer[i] == value:
            count += 1
    return count/len(Result)

def Test(I,W,Rnum): #For full input data, weighting on one generation, and give fitness of weighting
    R = []
    for i in I:
        temp = ML(i,W,Rnum)
        R.append(temp.index(max(temp)))
        
    Fitness = Assess(R, Answer)
    return Fitness

def Initiate(Sample_num):   #Make first weighting list for sample_num
    W_list = []
    global I
    for i in range(Sample_num):
        W = Weighting(len(I[0]),Rnum)
        W_list.append(W)
    return W_list

def Genetic(I,W_list,Rnum,Sample_num):
    Fitness_list = []
    for W in W_list:
        Fitness_list.append(Test(I,W,Rnum))
    return Fitness_list
    
def Next(W_list,Fitness_list,Sample_num,mutation_rate):   #By considerating fitness, make next generation of weighting
    Result = []
    for i in range(Sample_num):
        Temp = []
        for j in range(len(W_list[0])):
            Temp.append([])
            for k in range(len(W_list[0][0])):
                Gene = random.choices(range(Sample_num), weights=Fitness_list)[0]
                original = W_list[Gene][j][k]
                Temp[j].append(np.random.normal(original,abs(original)*mutation_rate,1)[0])
        Result.append(Temp)

    return Result

def Main(I,Rnum,Sample_num,Generation_num,mutation_rate, top_limit = 1.0, similar_limit = 1.0):
    W_list = Initiate(Sample_num)
    Fitness_list = Genetic(I,W_list,Rnum,Sample_num)

    top_list = []
    avg_list = []
    sim_list = []
    top = max(Fitness_list)
    avg = sum(Fitness_list)/len(Fitness_list)
    sim = avg/top
    print("{} {:.2f} {:.2f} {:.2f}%".format(0,top,avg,sim*100))
    top_list.append(top)
    avg_list.append(avg)
    sim_list.append(sim)

    gen = 0
    for i in range(Generation_num):
        gen+=1
        if sim > similar_limit:
            print("No drive")
            if top >= top_limit:
                print("Enough")
                break
            else:
                print("Reset")
                Main(I,Rnum,Sample_num,Generation_num,mutation_rate, top_limit = top_limit, similar_limit = similar_limit)
                return
        W_list = Next(W_list,Fitness_list,Sample_num,mutation_rate)
        Fitness_list = Genetic(I,W_list,Rnum,Sample_num)

        top = max(Fitness_list)
        avg = sum(Fitness_list)/len(Fitness_list)
        sim = avg/top
        print("{} {:.2f} {:.2f} {:.2f}%".format(i+1,top,avg,sim*100))
        top_list.append(top)
        avg_list.append(avg)
        sim_list.append(sim)

        selection = [[W_list[i],value] for i,value in enumerate(Fitness_list) if value < avg*0.7]  #Replace Weighting sample that has fitness below 70% of avg with random sample
        for j in selection:
            W_list.remove(j[0])
            Fitness_list.remove(j[1])
        temp = Initiate(len(selection))
        W_list += temp
        Fitness_list += Genetic(I,temp,Rnum,Sample_num)
        print("Replace",len(selection),"sample")

    ind = Fitness_list.index(max(Fitness_list))
    W = W_list[ind]
    data = []
    for i in range(len(df)):
        temp = list(df.iloc[i,:])
        data.append(temp)
    R = []
    global Result
    for i,value in enumerate(data):
        temp = ML(value[:-1],W,Rnum)
        r = temp.index(max(temp))
        R.append(Translator[r] == value[-1])
    
    print(W)
    print(R.count(True)/len(R))

    plt.plot(top_list)
    plt.plot(avg_list)
    plt.xlabel('Generation')
    plt.ylabel('Fitness')
    plt.xlim(0,gen)

    plt.ylim(0.0,1.0)
    plt.show()
    
Sample_num = 10000
print(len(I),len(I[0]),Rnum)
print("Sample_num:",Sample_num)
Main(I,Rnum,Sample_num,10,1.5,top_limit = 0.7,similar_limit = 0.73)
```
실제로 모델을 만들고 학습시키는 과정이다.

Main()에 parameter들을 넣어 호출함으로써 시작된다. Sample_num, 즉 한 세대를 구성하는 모델 개체의 수만큼 랜덤한 가중치 W matrix를 만들어준다. 이 모델들을 각각 I matrix와 곱해 예측 결과 matrix를 얻는다.   
각 예측 결과 matrix를 Answer 벡터와 비교해 정확하게 맞힌 비율을 fitness로 삼아, 모델별로 이 값을 측정하고 기록한다.   
모델들의 fitness의 최고치 top과 평균치 avg를 계산하고 top을 avg로 나눠 sim을 계산한다. sim은 모델들간의 유사도를 나타내는 지표로 활용했다. 이 값이 일정 이상으로 높아지면 각 세대를 이루는 모델들이 너무 비슷해져 사실상 개선이 되지 않기 때문이다.   
그래서 similar_limit을 설정하고 sim이 이 값을 넘어선 경우, 목표 정확도에 도달했으면 바로 종료하고 이에 미치지 못하면 프로그램을 재시작해 초기값을 새로 만들어준다.   
sim이 기준치보다 낮다면 다음 세대의 모델들을 구성하는데, 가중치 matrix의 각 원소마다 전체 모델들의 값 중 하나를 랜덤 추출해 대입한다. 이때 fitness가 높은 모델일수록 추출할 때 높은 확률로 선택된다.   
추출한 값을 대입하는 과정에 mutation이 반영된다. 값을 그대로 대입하지 않고 mutation_rate 값에 비례하는 표준편차를 적용해 정규분포를 만들고 이 분포에서 값을 추출해 대입한다.   
새롭게 구성된 세대들 중 fitness가 평균의 70%보다 낮은 개체들은 제거하고 랜덤한 개체로 대체한다.
위와 같은 유전 과정을 Generation_num만큼 반복 후 fitness가 가장 높은 모델 개체의 정보와 각 세대별 top,avg 값을 그래프로 그리면 프로그램이 종료된다.   

조건을 바꿔가면서 2번 실행해봤는데, 결과는 아래와 같았다.   
<p align = center>
    <img src="https://user-images.githubusercontent.com/62535139/212846473-a542e03f-cdb5-43da-8eb9-0ef0830f66d8.png" width = "49%" align = center>
    <img src="https://user-images.githubusercontent.com/62535139/212846913-91e46d02-7c37-4835-b68a-61b83a03d13a.png" width = "49%" align = center>
</p>    
왼쪽은 위 코드를 그대로 돌린 결과이고, 오른쪽은 다른 조건은 유지한 채 학습 데이터만 전체의 70%로 설정한 결과이다. 왼쪽의 경우 4세대에 sim 제한을 넘어 조기 종료되었다.  
제일 눈에 띄는 점은 사실상 세대가 지난다고 큰 변화가 나타나지 않는다는 것이다. 최대 10세대로 제한해서 영향이 안 나타난 것으로 볼 수도 있으나, 첫 세대부터 sim 값이 65를 넘고, 매 세대마다 sim 값은 증가하는 경향을 보이기에 어차피 계속 비슷한 값이 나오다가 프로그램이 종료된다.   
따라서 처음 생성된 10000개의 모델 중 가장 정확도가 높은 것을 바로 택하는 것과 결과가 크게 다르지 않다. 이렇게 되는 원인은 W matrix가 너무 작기 때문으로 생각할 수 있다.   
표의 구조상 W는 기껏해야 6\*2 크기밖에 안 된다. 때문에 각 모델 개체가 가질 수 있는 다양성에 한계가 있는 것이다.   
사실 그 외에도 지금 보면 왜 이렇게 설계했지 싶은 부분들이 꽤 있다. 유전을 할 때 개체 단위로 선택하지 않고 각각의 가중치 원소 단위로 선택하는 점이나, mutation을 부여하는 방식, 표준 편차에 원래 가중치 값을 곱한 이유 등 뭔가 다양한 시도를 해 보긴 했는데 근거가 없는 느낌이다.   
설계부터 충분한 근거가 없었기에 학습이 제대로 안 되는 건 당연한 일일지도 모른다. 그래도 당시에는 이 구조를 어떻게 개선할 방법이 없을까 고민하면서 관련 논문들을 찾아보기도 했다.   
전공이 컴퓨터 공학이 아니라서 대부분 논문을 읽어도 이해가 안 되는 내용뿐이었지만, 학습을 시킬 때 여러 layer를 거치게 함으로써 성능을 개선했다는 부분이 눈에 들어왔다.   
그래서 이 아이디어를 바탕으로 입력값을 n차원의 벡터로 변환하고, 이 벡터를 다시 m차원의 결과 벡터로 변환하는 구조로 바꿔봤다. 이 내용은 3번 글에서 다룰 예정이다.

결과적으로 학습은 이루어지지 않았지만, 그래도 대략 70%의 정확도로 결과를 예측하는 모델은 얻었다. 이에 대해 간단히 살펴보고자 한다.   
|label|Passenger Class|Sex|Age|No of Siblings|No of parents or children on board|Passenger Fare|
|---|---|---|---|---|---|---|
|Yes|0.00|-0.25|0.14|-3.57|1.05|7.15|
|No|-0.04|5.89|0.40|-2.03|-0.14|0.05|
위 표는 프로그램 실행 후 가장 높은 정확도를 나타낸 모델의 각 요소에 대한 가중치이다. 입력값에 각 가중치를 곱한 합을 바탕으로 Yes/No의 예측값을 구하고, 둘 중 더 높은 값을 예측 결과로 제시한다.   
생존자로 판단할 때 가장 중요한 요소는 승선권의 가격이었다. 비싼 티켓을 산 고위층 승객일수록 생존 확률이 높았음을 의미하지 않나 싶다.   
반대로 사망자로 판단할 때는 성별이 중요했다. 입력값에서 여자는 0, 남자는 1이므로 여자에 비해 남자의 사망 확률이 높다고 예측했다는 것이다. 노약자와 여성들의 구조 우선 순위가 더 높았기 때문이지 않나 싶다.   
형제의 수는 두 경우 모두에서 비슷한 수준의 중요도를 가지기에 결과를 구분하는 데에는 큰 영향이 없을 듯하다.   
