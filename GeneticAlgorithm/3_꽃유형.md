# 꽃유형
```python3
import pandas as pd
import random
import matplotlib.pyplot as plt

class Layer:
    def __init__(self,len_I,len_R):
        W = []
        for i in range(len_R):
            temp = []
            for j in range(len_I):
                temp.append(random.random())
            W.append(temp)
        self.W = W
        self.next = W

    def Weighting(self,I):
        R = []
        for i in self.W:
            temp = 0
            for index, value in enumerate(i):
                temp += I[index]*value
            R.append(temp)
            
        return R
                
def main():
    df = pd.read_excel('Clustering.xlsx',sheet_name='Sheet1')
    print(df.columns.tolist())
    label = input("Result label:")

    answer = []
    for i in df:
        if not i == label:
            data = df[i].tolist()
            max_value = max(data)
            df[i]/=max_value
        else:
            for j in df[i].tolist():
                if not j in answer:
                    answer.append(j)
    #normalize and make answer list
    
    N = int(input("population size:"))
    node_num = input("node number of each layer (csv):").split(",")
    for i, value in enumerate(node_num):
        node_num[i] = int(value)
    node_num = [len(df.columns.tolist())-1]+node_num+[len(answer)]

    pop = []
    for i in range(N):
        #Network making
        network = []
        for j, value in enumerate(node_num[:-1]):
            network.append(Layer(value,node_num[j+1]))

        pop.append(network)
    #Population is made

    example = random.sample(range(len(df)), int(len(df)*float(input("ratio for learning:"))))

    generation = int(input("Generation:"))

    top = []
    per = 0
    for g in range(generation+1):
        now = int((g+1)/(generation+1)*100)
        if now > per:
            print("{}% complete".format(per))
            per = now
        score = []
        for i in pop:
            #i is network
            count = 0
            for j in example:
                #j is index of data in dataframe
                data = df.iloc[j,:]
                node = data[:-1]
                for k in i:
                    #k is Layer
                    node = k.Weighting(node)
                result = node.index(max(node))
                #make decision for each data

                if result == answer.index(data[-1]):
                    count += 1
            score.append(count)

        #make next generation
        for network in pop:
            for j, layer in enumerate(network):
                next_W = []
                parents = random.choices(pop, weights = score, k = len(layer.W))
                for k, network_p in enumerate(parents):
                    next_W.append(network_p[j].W[k])
                network[j].next = next_W

        #update W to next
        for network in pop:
            for j in range(len(network)):
                network[j].W = network[j].next

        top.append(max(score))

    plt.plot(range(generation+1),top,label='top score')
    plt.xlim([0,generation])
    plt.ylim([0,len(example)])
    plt.legend()
    plt.show()

main()
```
앞서 만든 타이타닉과 비슷한 모델이다. 학습에 필요한 parameter들을 직접 입력받을 수 있도록 했다.  
가장 큰 차이점은 다중 레이어를 활용했다는 점이다. 학습률 개선을 위해 관련 논문을 찾아보다가 학습에 필요한 층을 여러 개 만드는 방법이 있음을 알게 됐다.  

아래는 각 세대별로 학습에 대해 최고점을 기록한 그래프이다.
![ML2 Figure_2](https://user-images.githubusercontent.com/62535139/230758122-7b55c406-5e8b-47de-8692-51fe49318823.png)  
세대 크기는 1000, 세대 수는 50으로 설정했고 전체 데이터의 70%를 학습에 사용했다. 학습 레이어를 3층으로 구성했으며 각각 10개, 7개, 5개의 노드를 갖도록 했다.  

결과 그래프를 보면 딱히 학습이 이뤄지지 않고 평가 점수가 오르락 내리락하는 것을 볼 수 있다.  
데이터 구성이 학습에 적절하지 않았던 것이 주 원인으로 생각된다.  
앞서 타이타닉 데이터와 달리 이번 데이터는 꽃유형, 꽃잎길이, 꽃잎너비 등 5가지의 특성만으로 이뤄져있다.  
결과 label인 꽃유형을 제외하면 사실상 4개의 특성으로 꽃을 분류해야 한다.  
학습에 사용할 특성 수 자체가 적다보니 각각에 가중치를 가해 합하는 이런 방식의 학습은 효과를 보기 힘들었던 듯하다.  
